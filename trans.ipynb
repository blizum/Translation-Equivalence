{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.meteor_score import single_meteor_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>Institute of affiliation</th>\n",
       "      <th>Current education level</th>\n",
       "      <th>moi kailoi dilliloi zam</th>\n",
       "      <th>xihotor kukurtu'e gutei ghorkhon letera kori pelai</th>\n",
       "      <th>toi bor bhal cricket khelo</th>\n",
       "      <th>tai muk eta dhuniya sula dile</th>\n",
       "      <th>ami aji cinema saam</th>\n",
       "      <th>...</th>\n",
       "      <th>etiya eyate norokhiba</th>\n",
       "      <th>ami xomoyot goi pua tu zoruri</th>\n",
       "      <th>parile mur karone kiba khabole loi ahibo</th>\n",
       "      <th>bhoipadura nohobi</th>\n",
       "      <th>moi zanibo palu ze aji boroxun dibo</th>\n",
       "      <th>kobar pora bohut beya gundho eta ahise</th>\n",
       "      <th>eitu zana kotha ze aakax khon neela hoi</th>\n",
       "      <th>eyate bor dangor pukhuri eta ase</th>\n",
       "      <th>xunili ne, amar suburit cinema shooting hobo?</th>\n",
       "      <th>bahirot dhumuha di ase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-11-16 22:47:36.479</td>\n",
       "      <td>Mon</td>\n",
       "      <td>24</td>\n",
       "      <td>JNU</td>\n",
       "      <td>Post Graduate</td>\n",
       "      <td>I will go to Delhi tomorrow</td>\n",
       "      <td>Their dog makes the whole house dirty</td>\n",
       "      <td>You play cricket really well</td>\n",
       "      <td>She gave me a nice shirt</td>\n",
       "      <td>We will watch movies today</td>\n",
       "      <td>...</td>\n",
       "      <td>Don't wait here now</td>\n",
       "      <td>We should reach on time</td>\n",
       "      <td>If possible, please bring me something to eat</td>\n",
       "      <td>Don't be a coward</td>\n",
       "      <td>I got to know that it'll rain today</td>\n",
       "      <td>There is a very bad smell coming from somewhere</td>\n",
       "      <td>It is known that the sky is blue</td>\n",
       "      <td>There's a very big pond here</td>\n",
       "      <td>Did you hear there's going to be a cinema shoo...</td>\n",
       "      <td>There's a storm blowing outside</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-11-16 23:42:37.404</td>\n",
       "      <td>Neil M Goswami</td>\n",
       "      <td>24</td>\n",
       "      <td>NLU Assam (2023)</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>I will go to Delhi tomorrow</td>\n",
       "      <td>Their dog littered the entire house</td>\n",
       "      <td>You play cricket very well</td>\n",
       "      <td>She gifted me a beautiful shirt</td>\n",
       "      <td>We will watch a movie today</td>\n",
       "      <td>...</td>\n",
       "      <td>Don’t stop here now</td>\n",
       "      <td>It’s important to react on time</td>\n",
       "      <td>Please bring me something to eat if you can</td>\n",
       "      <td>Don’t be a scared person</td>\n",
       "      <td>I got to know that it’s going to rain today</td>\n",
       "      <td>There is a very bad smell from somewhere</td>\n",
       "      <td>Its a know thing that the sky is blue</td>\n",
       "      <td>There is a big pond here</td>\n",
       "      <td>Have you heard, there is a movie being shot in...</td>\n",
       "      <td>There is a storm outside</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-11-16 23:47:23.247</td>\n",
       "      <td>Dev Darshan Mahanta</td>\n",
       "      <td>26</td>\n",
       "      <td>Christ University</td>\n",
       "      <td>Post Graduate</td>\n",
       "      <td>I'll go to Delhi tomorrow.</td>\n",
       "      <td>Their dog litters the whole house.</td>\n",
       "      <td>You play cricket very well.</td>\n",
       "      <td>She gave me a beautiful shirt.</td>\n",
       "      <td>We are going to watch a movie today.</td>\n",
       "      <td>...</td>\n",
       "      <td>Don't wait here now.</td>\n",
       "      <td>It is necessary for us to reach in time.</td>\n",
       "      <td>Bring something to eat for me if you can.</td>\n",
       "      <td>Don't be a coward.</td>\n",
       "      <td>I got to know that it is going to rain today.</td>\n",
       "      <td>There is a very foul smell coming from somewhe...</td>\n",
       "      <td>It is an obvious fact that the sky is blue.</td>\n",
       "      <td>There is a huge pond here.</td>\n",
       "      <td>Did you here that? There's going to be a cinem...</td>\n",
       "      <td>There is a storm outside.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Timestamp                  Name  Age Institute of affiliation  \\\n",
       "0 2024-11-16 22:47:36.479                   Mon   24                      JNU   \n",
       "1 2024-11-16 23:42:37.404        Neil M Goswami   24         NLU Assam (2023)   \n",
       "2 2024-11-16 23:47:23.247  Dev Darshan Mahanta    26       Christ University    \n",
       "\n",
       "  Current education level       moi kailoi dilliloi zam  \\\n",
       "0           Post Graduate   I will go to Delhi tomorrow   \n",
       "1                Graduate  I will go to Delhi tomorrow    \n",
       "2           Post Graduate   I'll go to Delhi tomorrow.    \n",
       "\n",
       "  xihotor kukurtu'e gutei ghorkhon letera kori pelai  \\\n",
       "0              Their dog makes the whole house dirty   \n",
       "1                Their dog littered the entire house   \n",
       "2                Their dog litters the whole house.    \n",
       "\n",
       "     toi bor bhal cricket khelo    tai muk eta dhuniya sula dile  \\\n",
       "0  You play cricket really well         She gave me a nice shirt   \n",
       "1    You play cricket very well  She gifted me a beautiful shirt   \n",
       "2  You play cricket very well.   She gave me a beautiful shirt.    \n",
       "\n",
       "                     ami aji cinema saam  ...  etiya eyate norokhiba  \\\n",
       "0             We will watch movies today  ...    Don't wait here now   \n",
       "1            We will watch a movie today  ...    Don’t stop here now   \n",
       "2  We are going to watch a movie today.   ...  Don't wait here now.    \n",
       "\n",
       "               ami xomoyot goi pua tu zoruri  \\\n",
       "0                    We should reach on time   \n",
       "1            It’s important to react on time   \n",
       "2  It is necessary for us to reach in time.    \n",
       "\n",
       "        parile mur karone kiba khabole loi ahibo          bhoipadura nohobi  \\\n",
       "0  If possible, please bring me something to eat          Don't be a coward   \n",
       "1    Please bring me something to eat if you can  Don’t be a scared person    \n",
       "2     Bring something to eat for me if you can.         Don't be a coward.    \n",
       "\n",
       "              moi zanibo palu ze aji boroxun dibo  \\\n",
       "0             I got to know that it'll rain today   \n",
       "1     I got to know that it’s going to rain today   \n",
       "2  I got to know that it is going to rain today.    \n",
       "\n",
       "              kobar pora bohut beya gundho eta ahise  \\\n",
       "0    There is a very bad smell coming from somewhere   \n",
       "1          There is a very bad smell from somewhere    \n",
       "2  There is a very foul smell coming from somewhe...   \n",
       "\n",
       "        eitu zana kotha ze aakax khon neela hoi  \\\n",
       "0              It is known that the sky is blue   \n",
       "1         Its a know thing that the sky is blue   \n",
       "2  It is an obvious fact that the sky is blue.    \n",
       "\n",
       "  eyate bor dangor pukhuri eta ase  \\\n",
       "0     There's a very big pond here   \n",
       "1         There is a big pond here   \n",
       "2      There is a huge pond here.    \n",
       "\n",
       "       xunili ne, amar suburit cinema shooting hobo?  \\\n",
       "0  Did you hear there's going to be a cinema shoo...   \n",
       "1  Have you heard, there is a movie being shot in...   \n",
       "2  Did you here that? There's going to be a cinem...   \n",
       "\n",
       "            bahirot dhumuha di ase  \n",
       "0  There's a storm blowing outside  \n",
       "1        There is a storm outside   \n",
       "2       There is a storm outside.   \n",
       "\n",
       "[3 rows x 35 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "as_df = pd.read_excel(\"Assamese Translations (Responses).xlsx\")\n",
    "as_df = as_df.drop(10)\n",
    "as_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_types = [\"Declarative\", \"Passive\", \"Interrogative\", \"Imperative\", \"Expletive\"]\n",
    "references = [\"I will go to Delhi tomorrow.\", \"Their dog litters the whole house.\", \"You play cricket very well.\",\n",
    "              \"She gave me a beautiful shirt.\", \"We will watch a movie today.\", \"His brother used to run daily in the morning.\",\n",
    "              \"The dog was killed by him.\", \"A new building is being constructed here.\", \"The house has been cleaned.\",\n",
    "              \"The door was closed by Sheela.\", \"This book was borrowed from the library.\", \"I am not being able to study.\",\n",
    "              \"How many people work there?\", \"You love dancing a lot, don't you?\", \"Are you going to the school?\",\n",
    "              \"By when will the doctor arrive?\", \"Isn’t she a famous singer?\", \"Where does he/she live?\",\n",
    "              \"Please turn on the fan.\", \"Listen, give me back my book.\", \"Don't wait here now.\",\n",
    "              \"It’s important for us to reach on time.\", \"If possible, bring something for me to eat.\", \"Don’t be a coward.\",\n",
    "              \"I got to know that it will rain today.\", \"There is a very foul smell coming from somewhere.\", \"It is a known fact that the sky is blue.\",\n",
    "              \"There is a big pond here.\", \"Did you hear that there will be a film shooting in our neighborhood?\", \"There is a storm outside.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_formatter(df):\n",
    "    data = []\n",
    "    \n",
    "    sentence_columns = df.iloc[::, 5:]\n",
    "    sentence_per_type = 6\n",
    "    for _, row in df.iterrows():\n",
    "        for i, sentence_column in enumerate(sentence_columns):\n",
    "            # Determine the sentence type based on column index\n",
    "            sentence_type_index = i // sentence_per_type\n",
    "            sentence_type = sentence_types[sentence_type_index]\n",
    "            \n",
    "            data.append({\n",
    "                \"Name\": row[\"Name\"],\n",
    "                \"Age\": row[\"Age\"],\n",
    "                \"Institute of Affiliation\": row[\"Institute of affiliation\"],\n",
    "                \"Current Education Level\": row[\"Current education level\"],\n",
    "                \"Sentence Type\": sentence_type,\n",
    "                \"Original\": sentence_column,\n",
    "                \"Reference\" : references[i],\n",
    "                \"English Translation\": row[sentence_column]\n",
    "            })\n",
    "    \n",
    "    data = pd.DataFrame(data)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "as_data = data_formatter(as_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>Institute of Affiliation</th>\n",
       "      <th>Current Education Level</th>\n",
       "      <th>Sentence Type</th>\n",
       "      <th>Original</th>\n",
       "      <th>Reference</th>\n",
       "      <th>English Translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mon</td>\n",
       "      <td>24</td>\n",
       "      <td>JNU</td>\n",
       "      <td>Post Graduate</td>\n",
       "      <td>Declarative</td>\n",
       "      <td>moi kailoi dilliloi zam</td>\n",
       "      <td>I will go to Delhi tomorrow.</td>\n",
       "      <td>I will go to Delhi tomorrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mon</td>\n",
       "      <td>24</td>\n",
       "      <td>JNU</td>\n",
       "      <td>Post Graduate</td>\n",
       "      <td>Declarative</td>\n",
       "      <td>xihotor kukurtu'e gutei ghorkhon letera kori p...</td>\n",
       "      <td>Their dog litters the whole house.</td>\n",
       "      <td>Their dog makes the whole house dirty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mon</td>\n",
       "      <td>24</td>\n",
       "      <td>JNU</td>\n",
       "      <td>Post Graduate</td>\n",
       "      <td>Declarative</td>\n",
       "      <td>toi bor bhal cricket khelo</td>\n",
       "      <td>You play cricket very well.</td>\n",
       "      <td>You play cricket really well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mon</td>\n",
       "      <td>24</td>\n",
       "      <td>JNU</td>\n",
       "      <td>Post Graduate</td>\n",
       "      <td>Declarative</td>\n",
       "      <td>tai muk eta dhuniya sula dile</td>\n",
       "      <td>She gave me a beautiful shirt.</td>\n",
       "      <td>She gave me a nice shirt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mon</td>\n",
       "      <td>24</td>\n",
       "      <td>JNU</td>\n",
       "      <td>Post Graduate</td>\n",
       "      <td>Declarative</td>\n",
       "      <td>ami aji cinema saam</td>\n",
       "      <td>We will watch a movie today.</td>\n",
       "      <td>We will watch movies today</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Name  Age Institute of Affiliation Current Education Level Sentence Type  \\\n",
       "0  Mon   24                      JNU           Post Graduate   Declarative   \n",
       "1  Mon   24                      JNU           Post Graduate   Declarative   \n",
       "2  Mon   24                      JNU           Post Graduate   Declarative   \n",
       "3  Mon   24                      JNU           Post Graduate   Declarative   \n",
       "4  Mon   24                      JNU           Post Graduate   Declarative   \n",
       "\n",
       "                                            Original  \\\n",
       "0                            moi kailoi dilliloi zam   \n",
       "1  xihotor kukurtu'e gutei ghorkhon letera kori p...   \n",
       "2                         toi bor bhal cricket khelo   \n",
       "3                      tai muk eta dhuniya sula dile   \n",
       "4                                ami aji cinema saam   \n",
       "\n",
       "                            Reference                    English Translation  \n",
       "0        I will go to Delhi tomorrow.            I will go to Delhi tomorrow  \n",
       "1  Their dog litters the whole house.  Their dog makes the whole house dirty  \n",
       "2         You play cricket very well.           You play cricket really well  \n",
       "3      She gave me a beautiful shirt.               She gave me a nice shirt  \n",
       "4        We will watch a movie today.             We will watch movies today  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "as_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Fraction.__new__() got an unexpected keyword argument '_normalize'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m as_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBLEU\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m as_data\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m row: sentence_bleu([row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReference\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msplit()], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnglish Translation\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msplit()), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      3\u001b[0m )\n\u001b[0;32m      4\u001b[0m as_data\n",
      "File \u001b[1;32mc:\\Users\\bhabo\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:10374\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m  10360\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[0;32m  10362\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[0;32m  10363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  10364\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10372\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m  10373\u001b[0m )\n\u001b[1;32m> 10374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mapply()\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\bhabo\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[1;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\bhabo\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_generator()\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[1;32mc:\\Users\\bhabo\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[0;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(v, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[59], line 2\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m      1\u001b[0m as_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBLEU\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m as_data\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m row: sentence_bleu([row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReference\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msplit()], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnglish Translation\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msplit()), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      3\u001b[0m )\n\u001b[0;32m      4\u001b[0m as_data\n",
      "File \u001b[1;32mc:\\Users\\bhabo\\anaconda3\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:107\u001b[0m, in \u001b[0;36msentence_bleu\u001b[1;34m(references, hypothesis, weights, smoothing_function, auto_reweigh)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentence_bleu\u001b[39m(\n\u001b[0;32m     46\u001b[0m     references,\n\u001b[0;32m     47\u001b[0m     hypothesis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     50\u001b[0m     auto_reweigh\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     51\u001b[0m ):\n\u001b[0;32m     52\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03m    Calculate BLEU score (Bilingual Evaluation Understudy) from\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;124;03m    Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;124;03m    \"BLEU: a method for automatic evaluation of machine translation.\"\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    In Proceedings of ACL. https://www.aclweb.org/anthology/P02-1040.pdf\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \n\u001b[0;32m     58\u001b[0m \u001b[38;5;124;03m    >>> hypothesis1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'which',\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;124;03m    ...               'ensures', 'that', 'the', 'military', 'always',\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m    ...               'obeys', 'the', 'commands', 'of', 'the', 'party']\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m    >>> hypothesis2 = ['It', 'is', 'to', 'insure', 'the', 'troops',\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03m    ...               'forever', 'hearing', 'the', 'activity', 'guidebook',\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03m    ...               'that', 'party', 'direct']\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03m    >>> reference1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'that',\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m    ...               'ensures', 'that', 'the', 'military', 'will', 'forever',\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m    ...               'heed', 'Party', 'commands']\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    >>> reference2 = ['It', 'is', 'the', 'guiding', 'principle', 'which',\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    ...               'guarantees', 'the', 'military', 'forces', 'always',\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03m    ...               'being', 'under', 'the', 'command', 'of', 'the',\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03m    ...               'Party']\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \n\u001b[0;32m     75\u001b[0m \u001b[38;5;124;03m    >>> reference3 = ['It', 'is', 'the', 'practical', 'guide', 'for', 'the',\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;124;03m    ...               'army', 'always', 'to', 'heed', 'the', 'directions',\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;124;03m    ...               'of', 'the', 'party']\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \n\u001b[0;32m     79\u001b[0m \u001b[38;5;124;03m    >>> sentence_bleu([reference1, reference2, reference3], hypothesis1) # doctest: +ELLIPSIS\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;124;03m    0.5045...\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03m    If there is no ngrams overlap for any order of n-grams, BLEU returns the\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m    value 0. This is because the precision for the order of n-grams without\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m    overlap is 0, and the geometric mean in the final BLEU score computation\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;124;03m    multiplies the 0 with the precision of other n-grams. This results in 0\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;124;03m    (independently of the precision of the other n-gram orders). The following\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;124;03m    example has zero 3-gram and 4-gram overlaps:\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \n\u001b[0;32m     89\u001b[0m \u001b[38;5;124;03m    >>> round(sentence_bleu([reference1, reference2, reference3], hypothesis2),4) # doctest: +ELLIPSIS\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;124;03m    0.0\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \n\u001b[0;32m     92\u001b[0m \u001b[38;5;124;03m    To avoid this harsh behaviour when no ngram overlaps are found a smoothing\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;124;03m    function can be used.\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \n\u001b[0;32m     95\u001b[0m \u001b[38;5;124;03m    >>> chencherry = SmoothingFunction()\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03m    >>> sentence_bleu([reference1, reference2, reference3], hypothesis2,\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;124;03m    ...     smoothing_function=chencherry.method1) # doctest: +ELLIPSIS\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    0.0370...\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    The default BLEU calculates a score for up to 4-grams using uniform\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;124;03m    weights (this is called BLEU-4). To evaluate your translations with\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;124;03m    higher/lower order ngrams, use customized weights. E.g. when accounting\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    for up to 5-grams with uniform weights (this is called BLEU-5) use:\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m    >>> weights = (1./5., 1./5., 1./5., 1./5., 1./5.)\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03m    >>> sentence_bleu([reference1, reference2, reference3], hypothesis1, weights) # doctest: +ELLIPSIS\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m \u001b[38;5;124;03m    0.3920...\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \n\u001b[0;32m    109\u001b[0m \u001b[38;5;124;03m    Multiple BLEU scores can be computed at once, by supplying a list of weights.\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;124;03m    E.g. for computing BLEU-2, BLEU-3 *and* BLEU-4 in one computation, use:\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    >>> weights = [\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    ...     (1./2., 1./2.),\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m    ...     (1./3., 1./3., 1./3.),\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;124;03m    ...     (1./4., 1./4., 1./4., 1./4.)\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;124;03m    ... ]\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m    >>> sentence_bleu([reference1, reference2, reference3], hypothesis1, weights) # doctest: +ELLIPSIS\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    [0.7453..., 0.6240..., 0.5045...]\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \n\u001b[0;32m    119\u001b[0m \u001b[38;5;124;03m    :param references: reference sentences\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;124;03m    :type references: list(list(str))\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;124;03m    :param hypothesis: a hypothesis sentence\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;124;03m    :type hypothesis: list(str)\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;124;03m    :param weights: weights for unigrams, bigrams, trigrams and so on (one or a list of weights)\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03m    :type weights: tuple(float) / list(tuple(float))\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;124;03m    :param smoothing_function:\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;124;03m    :type smoothing_function: SmoothingFunction\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    :param auto_reweigh: Option to re-normalize the weights uniformly.\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    :type auto_reweigh: bool\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    :return: The sentence-level BLEU score. Returns a list if multiple weights were supplied.\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    :rtype: float / list(float)\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m corpus_bleu(\n\u001b[0;32m    133\u001b[0m         [references], [hypothesis], weights, smoothing_function, auto_reweigh\n\u001b[0;32m    134\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\bhabo\\anaconda3\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:210\u001b[0m, in \u001b[0;36mcorpus_bleu\u001b[1;34m(list_of_references, hypotheses, weights, smoothing_function, auto_reweigh)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcorpus_bleu\u001b[39m(\n\u001b[0;32m    138\u001b[0m     list_of_references,\n\u001b[0;32m    139\u001b[0m     hypotheses,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    142\u001b[0m     auto_reweigh\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    143\u001b[0m ):\n\u001b[0;32m    144\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;124;03m    Calculate a single corpus-level BLEU score (aka. system-level BLEU) for all\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;124;03m    the hypotheses and their respective references.\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \n\u001b[0;32m    148\u001b[0m \u001b[38;5;124;03m    Instead of averaging the sentence level BLEU scores (i.e. macro-average\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;124;03m    precision), the original BLEU metric (Papineni et al. 2002) accounts for\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;124;03m    the micro-average precision (i.e. summing the numerators and denominators\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;124;03m    for each hypothesis-reference(s) pairs before the division).\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \n\u001b[0;32m    153\u001b[0m \u001b[38;5;124;03m    >>> hyp1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'which',\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;124;03m    ...         'ensures', 'that', 'the', 'military', 'always',\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;124;03m    ...         'obeys', 'the', 'commands', 'of', 'the', 'party']\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03m    >>> ref1a = ['It', 'is', 'a', 'guide', 'to', 'action', 'that',\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m    ...          'ensures', 'that', 'the', 'military', 'will', 'forever',\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03m    ...          'heed', 'Party', 'commands']\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;124;03m    >>> ref1b = ['It', 'is', 'the', 'guiding', 'principle', 'which',\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03m    ...          'guarantees', 'the', 'military', 'forces', 'always',\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;124;03m    ...          'being', 'under', 'the', 'command', 'of', 'the', 'Party']\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;124;03m    >>> ref1c = ['It', 'is', 'the', 'practical', 'guide', 'for', 'the',\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m    ...          'army', 'always', 'to', 'heed', 'the', 'directions',\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03m    ...          'of', 'the', 'party']\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \n\u001b[0;32m    166\u001b[0m \u001b[38;5;124;03m    >>> hyp2 = ['he', 'read', 'the', 'book', 'because', 'he', 'was',\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m    ...         'interested', 'in', 'world', 'history']\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;124;03m    >>> ref2a = ['he', 'was', 'interested', 'in', 'world', 'history',\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;124;03m    ...          'because', 'he', 'read', 'the', 'book']\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \n\u001b[0;32m    171\u001b[0m \u001b[38;5;124;03m    >>> list_of_references = [[ref1a, ref1b, ref1c], [ref2a]]\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;124;03m    >>> hypotheses = [hyp1, hyp2]\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;124;03m    >>> corpus_bleu(list_of_references, hypotheses) # doctest: +ELLIPSIS\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;124;03m    0.5920...\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \n\u001b[0;32m    176\u001b[0m \u001b[38;5;124;03m    The example below show that corpus_bleu() is different from averaging\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;124;03m    sentence_bleu() for hypotheses\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \n\u001b[0;32m    179\u001b[0m \u001b[38;5;124;03m    >>> score1 = sentence_bleu([ref1a, ref1b, ref1c], hyp1)\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;124;03m    >>> score2 = sentence_bleu([ref2a], hyp2)\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;124;03m    >>> (score1 + score2) / 2 # doctest: +ELLIPSIS\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m    0.6223...\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \n\u001b[0;32m    184\u001b[0m \u001b[38;5;124;03m    Custom weights may be supplied to fine-tune the BLEU score further.\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;124;03m    A tuple of float weights for unigrams, bigrams, trigrams and so on can be given.\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;124;03m    >>> weights = (0.1, 0.3, 0.5, 0.1)\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;124;03m    >>> corpus_bleu(list_of_references, hypotheses, weights=weights) # doctest: +ELLIPSIS\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;124;03m    0.5818...\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03m    This particular weight gave extra value to trigrams.\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;124;03m    Furthermore, multiple weights can be given, resulting in multiple BLEU scores.\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;124;03m    >>> weights = [\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;124;03m    ...     (0.5, 0.5),\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;124;03m    ...     (0.333, 0.333, 0.334),\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;124;03m    ...     (0.25, 0.25, 0.25, 0.25),\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;124;03m    ...     (0.2, 0.2, 0.2, 0.2, 0.2)\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;124;03m    ... ]\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;124;03m    >>> corpus_bleu(list_of_references, hypotheses, weights=weights) # doctest: +ELLIPSIS\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;124;03m    [0.8242..., 0.7067..., 0.5920..., 0.4719...]\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03m    :param list_of_references: a corpus of lists of reference sentences, w.r.t. hypotheses\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;124;03m    :type list_of_references: list(list(list(str)))\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;124;03m    :param hypotheses: a list of hypothesis sentences\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;124;03m    :type hypotheses: list(list(str))\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03m    :param weights: weights for unigrams, bigrams, trigrams and so on (one or a list of weights)\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m    :type weights: tuple(float) / list(tuple(float))\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m    :param smoothing_function:\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;124;03m    :type smoothing_function: SmoothingFunction\u001b[39;00m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;124;03m    :param auto_reweigh: Option to re-normalize the weights uniformly.\u001b[39;00m\n\u001b[1;32m--> 210\u001b[0m \u001b[38;5;124;03m    :type auto_reweigh: bool\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;124;03m    :return: The corpus-level BLEU score.\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;124;03m    :rtype: float\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# Before proceeding to compute BLEU, perform sanity checks.\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     p_numerators \u001b[38;5;241m=\u001b[39m Counter()  \u001b[38;5;66;03m# Key = ngram order, and value = no. of ngram matches.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bhabo\\anaconda3\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:368\u001b[0m, in \u001b[0;36mmodified_precision\u001b[1;34m(references, hypothesis, n)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodified_precision\u001b[39m(references, hypothesis, n):\n\u001b[0;32m    286\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;124;03m    Calculate modified ngram precision.\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \n\u001b[0;32m    289\u001b[0m \u001b[38;5;124;03m    The normal precision method may lead to some wrong translations with\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;124;03m    high-precision, e.g., the translation, in which a word of reference\u001b[39;00m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m    repeats several times, has very high precision.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \n\u001b[0;32m    293\u001b[0m \u001b[38;5;124;03m    This function only returns the Fraction object that contains the numerator\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;124;03m    and denominator necessary to calculate the corpus-level precision.\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;124;03m    To calculate the modified precision for a single pair of hypothesis and\u001b[39;00m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;124;03m    references, cast the Fraction object into a float.\u001b[39;00m\n\u001b[0;32m    297\u001b[0m \n\u001b[0;32m    298\u001b[0m \u001b[38;5;124;03m    The famous \"the the the ... \" example shows that you can get BLEU precision\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;124;03m    by duplicating high frequency words.\u001b[39;00m\n\u001b[0;32m    300\u001b[0m \n\u001b[0;32m    301\u001b[0m \u001b[38;5;124;03m        >>> reference1 = 'the cat is on the mat'.split()\u001b[39;00m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;124;03m        >>> reference2 = 'there is a cat on the mat'.split()\u001b[39;00m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;124;03m        >>> hypothesis1 = 'the the the the the the the'.split()\u001b[39;00m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;124;03m        >>> references = [reference1, reference2]\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;124;03m        >>> float(modified_precision(references, hypothesis1, n=1)) # doctest: +ELLIPSIS\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;124;03m        0.2857...\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \n\u001b[0;32m    308\u001b[0m \u001b[38;5;124;03m    In the modified n-gram precision, a reference word will be considered\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    exhausted after a matching hypothesis word is identified, e.g.\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \n\u001b[0;32m    311\u001b[0m \u001b[38;5;124;03m        >>> reference1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'that',\u001b[39;00m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;124;03m        ...               'ensures', 'that', 'the', 'military', 'will',\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;124;03m        ...               'forever', 'heed', 'Party', 'commands']\u001b[39;00m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;124;03m        >>> reference2 = ['It', 'is', 'the', 'guiding', 'principle', 'which',\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;124;03m        ...               'guarantees', 'the', 'military', 'forces', 'always',\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;124;03m        ...               'being', 'under', 'the', 'command', 'of', 'the',\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;124;03m        ...               'Party']\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;124;03m        >>> reference3 = ['It', 'is', 'the', 'practical', 'guide', 'for', 'the',\u001b[39;00m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;124;03m        ...               'army', 'always', 'to', 'heed', 'the', 'directions',\u001b[39;00m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;124;03m        ...               'of', 'the', 'party']\u001b[39;00m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;124;03m        >>> hypothesis = 'of the'.split()\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;124;03m        >>> references = [reference1, reference2, reference3]\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;124;03m        >>> float(modified_precision(references, hypothesis, n=1))\u001b[39;00m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;124;03m        1.0\u001b[39;00m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;124;03m        >>> float(modified_precision(references, hypothesis, n=2))\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;124;03m        1.0\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \n\u001b[0;32m    328\u001b[0m \u001b[38;5;124;03m    An example of a normal machine translation hypothesis:\u001b[39;00m\n\u001b[0;32m    329\u001b[0m \n\u001b[0;32m    330\u001b[0m \u001b[38;5;124;03m        >>> hypothesis1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'which',\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;124;03m        ...               'ensures', 'that', 'the', 'military', 'always',\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;124;03m        ...               'obeys', 'the', 'commands', 'of', 'the', 'party']\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m        >>> hypothesis2 = ['It', 'is', 'to', 'insure', 'the', 'troops',\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;124;03m        ...               'forever', 'hearing', 'the', 'activity', 'guidebook',\u001b[39;00m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m        ...               'that', 'party', 'direct']\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \n\u001b[0;32m    338\u001b[0m \u001b[38;5;124;03m        >>> reference1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'that',\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m        ...               'ensures', 'that', 'the', 'military', 'will',\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;124;03m        ...               'forever', 'heed', 'Party', 'commands']\u001b[39;00m\n\u001b[0;32m    341\u001b[0m \n\u001b[0;32m    342\u001b[0m \u001b[38;5;124;03m        >>> reference2 = ['It', 'is', 'the', 'guiding', 'principle', 'which',\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;124;03m        ...               'guarantees', 'the', 'military', 'forces', 'always',\u001b[39;00m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;124;03m        ...               'being', 'under', 'the', 'command', 'of', 'the',\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;124;03m        ...               'Party']\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \n\u001b[0;32m    347\u001b[0m \u001b[38;5;124;03m        >>> reference3 = ['It', 'is', 'the', 'practical', 'guide', 'for', 'the',\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;124;03m        ...               'army', 'always', 'to', 'heed', 'the', 'directions',\u001b[39;00m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;124;03m        ...               'of', 'the', 'party']\u001b[39;00m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;124;03m        >>> references = [reference1, reference2, reference3]\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;124;03m        >>> float(modified_precision(references, hypothesis1, n=1)) # doctest: +ELLIPSIS\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;124;03m        0.9444...\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;124;03m        >>> float(modified_precision(references, hypothesis2, n=1)) # doctest: +ELLIPSIS\u001b[39;00m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;124;03m        0.5714...\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;124;03m        >>> float(modified_precision(references, hypothesis1, n=2)) # doctest: +ELLIPSIS\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;124;03m        0.5882352941176471\u001b[39;00m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;124;03m        >>> float(modified_precision(references, hypothesis2, n=2)) # doctest: +ELLIPSIS\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;124;03m        0.07692...\u001b[39;00m\n\u001b[0;32m    359\u001b[0m \n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m    :param references: A list of reference translations.\u001b[39;00m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;124;03m    :type references: list(list(str))\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;124;03m    :param hypothesis: A hypothesis translation.\u001b[39;00m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;124;03m    :type hypothesis: list(str)\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;124;03m    :param n: The ngram order.\u001b[39;00m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;124;03m    :type n: int\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;124;03m    :return: BLEU's modified precision for the nth order ngram.\u001b[39;00m\n\u001b[1;32m--> 368\u001b[0m \u001b[38;5;124;03m    :rtype: Fraction\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;66;03m# Extracts all ngrams in hypothesis\u001b[39;00m\n\u001b[0;32m    371\u001b[0m     \u001b[38;5;66;03m# Set an empty Counter if hypothesis is empty.\u001b[39;00m\n\u001b[0;32m    372\u001b[0m     counts \u001b[38;5;241m=\u001b[39m Counter(ngrams(hypothesis, n)) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(hypothesis) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m n \u001b[38;5;28;01melse\u001b[39;00m Counter()\n",
      "\u001b[1;31mTypeError\u001b[0m: Fraction.__new__() got an unexpected keyword argument '_normalize'"
     ]
    }
   ],
   "source": [
    "as_data['BLEU'] = as_data.apply(\n",
    "    lambda row: sentence_bleu([row['Reference'].split()], row['English Translation'].split()), axis=1\n",
    ")\n",
    "as_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data['Original'][2].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
